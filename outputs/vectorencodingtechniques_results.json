{
  "status": "success",
  "media_file": "https://youtu.be/Kl--C14yn6g?si=0WX_wfH5FxzAYrbk",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_6kwf4k8c/AI_Engineering_2_Vector_Embeddings_for_LLMs_youtube.wav",
  "transcript": "till 9.25 before we start with vector encodings. I tell you the agenda of this, the basic idea is you will by the end of this know what vectors are, you will know how they are created and you will know some more detail about the transform architecture. We have just scratched the surface of transformers, we will get into more detail this time. And the best way to semantically understand unstructured data, it seems like yes, vectors are basically compressed representations of concepts. When I say concepts, it can be objects. So, you have an event and it has a time stamp, it has some properties, some parameters. If you take the hash of this entire thing, then you would expect a single number. And if you take the hash of the same, I mean the event with the same properties and the same time stamp, you would expect the same number. You can think of vectors in a similar way. Vectors, when you pass in an object or when you pass in some sort of text, it converts it into a 1D array representation, where every value in this array in this vector corresponds to some sort of meaning. So, I am taking an example here, this is not exactly how it happens, but if I take an example of a person and I say, let me convert them into a vector, in the context of trying to find how hireable they are job hiring, I will say how much technical skill do they have. So, I give them a 0.9 score here. I say how expensive is it to hire them? So, what is their salary or how much compensation are they expecting? So, I give them a very high score here, 0.92. Then I say how easy are they to work with? Have they previously worked in companies for more than five years? So, they are the chance of staying in this company and so on. If I represent them as a single vector, then I can encapsulate the underlying meaning of the person in this context of job hiring very efficiently. The person who is equally good, most of the things they are similar will see two similar vectors. And finally, let us say there is a third person who is very young, who is still learning. This person is going to have a different kind of vector, they are going to have something like person three. Their technical skill right now is improving. So, they are at a 0.3. Their cost of hiring is also very low, 0.1, because they are just fresh out of college, let us say. And then how easy are they to work with? Maybe 0.8 and so on. But if you take the distance between these two vectors and these two vectors, you will see that these two are large. And you will see the distance between these two vectors is small. Let me use a different color. Small distance, large distance. Yes, that is a very good example, infinitely DDO, which is example, we can parse a person's resume. So, imagine you have a large language model, which takes different people's resumes and then starts converting them into vectors, converting them into these 1D areas where like in an Excel sheet, you have things like what is their expected salary, what is their years of experience, how many references do they have, are people saying good things about them and so on. And then this Excel sheet is basically converted into points in an n dimensional space. So, you have person one here, person two here and person three quite far. And so now you might say I want a criteria where only people with high experience will be taken. So, then you basically filter out person three. And then you say you know what, person one, I really like this person, but I am hiring five people. Can you get me all people who are similar? So, then you do a distance measure from person one and the next four people are picked up in total you have five, you hire all of them or you at least interview all of them. So, this would be a way to do it. You can also have a dummy person here, it is just a representation. You say this is the ideal person for this job, find me the people closest to this ideal, this into embeddings. Okay, good stuff. Let us look at vectors at a deeper level, especially how they are created. We know what vectors are, let us see how they are created. Let me refer to this diagram from attention is all you need. It is a very useful paper, but everyone here sees an input embedding. So, look at this, this is inputs. What do you think these are? Can someone quickly tell me in the chat what are inputs for the last language model? But that happens at attention. You can say context is the tokens also input chunks of data. Karthik, yes, you are very close. Tokens is reason for that is because when you have, let us take the, let us take a new example. Let us say I am eating a pancake. Okay, looking at this, most humans will look at this and say I have added a space here, but most humans will still say that I am eating a pancake. It is not that I am eating a cake made of pans. This would be incorrect. I will be looking at one thing. What I am going to do then is convert these words. I also have a smart way to do this. I is going to be converted into a vector. We saw how vectors are being created. It's basically like an Excel sheet of features, different features, numerical data. Am gets converted to another vector V2. Eating is V3. This is V4. And then we get pan, which is V5 and cake, which is V6. We have two options. One is to convert this into a single token. And the second is to keep these as two different tokens with two different vectors and let the last language model handle it. What do you think makes more sense? Should this be just a single token V5 or should this be V5 and V6? Sparse, if we do it that way, yes, is saying that if we have it as single characters, it will become sparse. So the number is saying space separated words. So go for separate tokens. The tokenization process to understand it well goes like this. It actually converts all of the input text into CARES characters. So you literally have I, space, A, M, space, E, A, T, I, N, G, space and so on. What happens in the next step is we look at the amount of info that can be derived sequences, right? Sequence of characters. So let's say we have I and we have space. Space, let's say is a separator character for simplicity. If I go through thousands of documents, I will see that A and M occur very frequently. So I'll maintain a dictionary of characters which occur frequently. A and M occur 50,000 times together. Out of the, okay, yeah, they occur 50,000 times together. So I am going to now start assuming that am is actually a word. I put them together, I pair them together and now I have a single word am. What about EA? EA is also occurring together very often. What about EAT? The next time when I go through, when I pass this, so let's say I take EA and I still have the other characters. So, it comes to I am EA, T, I, N, G. These are all separate. Converts to I am EAT, I, N, G. Should I merge these together? Should I make EATING a single word? I am EATING, I am PLING, I am JUMPING, I am DANCING. There's a lot of INGs there. Should I merge them with the original words? Should I have ING separate? I'll just go over it one more time because there was a big break. So, the idea is that if you are saying I am EATING in the input sequence, then we take the words I, am and EAT. EAT is put together, the characters are all put together. Initially, we have all of them separated out. We start putting them together. The moment we see EAT, we realize that this is an extremely common sequence. So we take that as a word. But when we say EAT and I, if we start taking all the possible ING after the verb, then this will become a 10,000 word vocabulary here of EAT, sing, dance and merging ING into them will create 10,000 new words of EATING, singing, dancing and so on. So in total, we'll have 20,000 words. That's a huge increase in the number of words. So instead, what we do is we take a single word of ING which represents doing something, right? The action is being done right now. That is what ING says. And what is the action is here? So the number of words, the number of unique vectors you have to deal with is reduced. Similarly, we took the example of engineer, developer, lawyer, painter. There's always an ER at the end of it, which means a person who does. So a developer develops, a lawyer practices law. So we take ER as a separate word and we take all of the root words separately. So this avoids an explosion of words in our input sequence. Yeah. So is this making sense? This is the reason why we have tokens not just as pure words, not space separated characters, but as most commonly occurring substrings. Tomorrow, if I say Amitabh and so this is a famous Indian actor and I say, oh my God, I think I've forgotten the spelling of Bachchan. But yes, if I say Amitabh, the chance of Bachchan coming ahead is quite high. If I say Brad, the chance of Brad Pitt coming together is quite high. So we might want to take this as a single word. Okay, because this is going to reduce our vocabulary size. That's crazy. But in the case of eat, we want to separate this out. Eat and eat. Because eating is occurring very often in other places. Eat is occurring very often in other places. That is what byte pair encoding is. Keep adding words till the entropy of your vocabulary goes down. Is there a separate algorithm to do vector embeddings? Yes. Until now, we have just done tokens. These tokens will now be converted into vectors. So Nikunj, let's say you have a dictionary and you have all these characters A, B, C, D, E, F, G, H, I. Now, if I take A and... What's the commonly... So I take A and P. Let's say Apple. If I take A and P together, I see that they have many common occurrences of A and P. So I'm increasing one word here, AP, just one word. But I'm significantly reducing the number of single occurrences of A and P. I'm putting it in one place. So I try to merge them together. At a higher level, if you have these leaves and you are trying to merge them into a root, then what you're interested in... We are doing it one by one. So this is just two of these substrings will be merged. If the root is going to introduce just one word, but the children frequency... Children's frequency is going to reduce significantly. The exact formula you can look into, byte pair encoding. It's a bit involved. But this is the high level idea. To reduce entropy. Okay. Let's move to the final bit, which is how are tokens being converted into vectors or how are vectors in general generated? So here I'm going to refer to BRT, BERT, which is bi-directional transformers for language understanding. Next, we are going to focus on the transformer side of things. Let's say you have these input tokens and you randomly convert them into vectors. Random. Okay. You just convert them into any gibberish. So, and the number of dimensions in this vector can be anything. Let's say 700 or 10,000. Doesn't matter. For now, you have random vectors. You pass this in to the attention mechanism. So it's able to massage the vectors such that they get contextual meaning. We saw this last time how that happened. The word bank gets closer to either river or a financial institution, depending on the other words in the sentence. Right. For now, you have given gibberish. So you will have gibberish attention also. Okay. Then the words are sent through a neural network. This is a feed forward neural network. But for our understanding, just think of a neural network, right? It has inputs. It has some connections internally. And finally it has outputs. At the end of these outputs, we are going to take all of our input tokens. We are going to take our input string, which was I am eating a pan cake. And the output should have all of those words again. I am eating a pan cake. Okay. If you are able to successfully reconstruct this sentence, we know that we are learning something. Okay. And to make it interesting and to actually make it sensible. I'm going to delete a word from the input sequence. I'm going to say I am blank up pan blank as a human. You will start guessing what the words here will be. I've already told you this is blank here and blank here. So you might say I am heating a pan strongly. Okay. The chance of you actually guessing that I'm eating a pan cake is also there, but may not be as high. This is reasonable. This is also reasonable. But saying I am crushing a pan tomb makes little sense. So at this point, this is bad output. This is good output. This is also good output. Is this making sense folks? What we did is take the input sequence, hide some tokens and ask the model to start predicting the original input sequence or just the hidden words. So if it says eating and cake.wonderful. That was exactly what was there in the original sentence, which we had hidden away. But if it says something else, we are going to punish it. We're going to punish it here. Okay. The weights will be updated based on their distance from the original statement and those will be updated using back propagation. Is this making sense? Standard neural network stuff. When an input is given to the neural network, you have an expected output. If the expected output is the same as the input, we make no changes. If the expected output is different from the actual output, we update the weights of the neural network through back propagation. So in our case, we take our input sentence, hide or mask some of the words, ask the model to predict the missing words. If it predicts correctly, we know that the vectors you created were quite good. If you predicted incorrectly, then we know that the weights of your feed forward neural network are bad. The vectors you created are quite bad. We're going to change both of them. Okay. Attention is basically, we'll get to that also. There's something called multi head attention where these weights also changed, but the entire architecture goes through a change based on how well it is performing for different inputs. Okay. So the neural network, which is generating these vectors, this is a small neural network here, which generated this vector is going to suffer back propagation. If it gets it wrong, if it generates poor vectors and even this is going to suffer, if it gets it right, we congratulate it and let the weights stay. This is how you can generate vectors because you will have billions of tokens being passed by this billions or at least millions of sentences. The entire Wikipedia text corpus can be sent through this Bert model. It's going to convert them into tokens, those sentences into tokens. Some of them will be hidden, but we'll try to predict what tokens should have been there and it's eventually going to get so good at it that you know that the input tokens are being converted into very good vectors. Okay. There's another thing that Bert does. It also says that, okay, given two input sentences, sentence one, I am eating a pancake. I was hungry. It's going to look at sentence one and sentence two and then predict the order of the sentences. Does sentence two occur before sentence one or sentence one occur before sentence two? This also helps it because what we do is we jumble the order. We sometimes put sentence one first, sentence two first, we jumble the order and we start asking him to predict. This simple exercise also improves its accuracy vector embeddings. It starts understanding causal sequence. So I am hungry. Therefore I ate a pancake makes more sense than I ate a pancake. Here there should have been a because I was hungry. Instead I was hungry. Doesn't make that much sense. So the model starts understanding what occurs after another. There is a causal understanding and there is a meaning understanding also through the vectors. Okay. I'll just change color here. Input tokens are being now converted into good vectors using this neural network, which goes through back propagation. Any questions, folks, on what vectors are, how they're being created right now? It's just a neural network. And the neural network specifically is in the transformer architecture. There's two parts to it. It's this left hand side that you see input embedding, gibberish, nonsense, doesn't make any sense initially. Of course you have tokens, but initially you come up with very horrible vectors. Those vectors then get some position encoding. We'll get to this later. It's basically like adding, you know, I am eating a pancake. So I take all the indexes of the inputs and I add that to the vector. Then I send it through attention. Initially if your vectors are horrible, then your attention is also horrible. Feed forward neural network again comes up with gibberish. But if you predict things wrong, if you predict the wrong tokens that have been masked, then you're going to suffer back propagation. And so you come back here, your input embedding, the weights here change and you keep going ahead and the weights keep changing till you start coming up with really good input embeddings and really good feed forward neural network. Is this making sense folks? Any questions? As software engineers who are transitioning to AI engineering, the reason this is useful is because we know exactly how these vectors are now being created. They are being converted into smaller representations of themselves using a neural network. And the neural network is being trained to start predicting what word should occur in a given sentence in which position. We are masking parts of the input statement here as we saw over here. And then the model is starting to predict well where those words or what word should come here and here. Tomorrow let me say that if I say I went to the mall to get groceries, I got it. And I start masking stuff out. So if I get rid of mall, then as humans you will know that this is either the shop or the mall or the market, but something which makes sense. And as a human, I can then teach you, no, no, no, no, no, no, I was talking about the mall here. If you get it wrong, I will tell you that this was not good, but if you get it right, then I am going to say that is wonderful. Please continue thinking the way you are. Okay folks, so what we have learnt in this session is what vectors are, how vectors are constructed using tokens, what tokens themselves are. And finally we have gone over how these vectors are being created because we went through transformers or attention last time. It was a bit easier, but yes, this is how they are created. Okay, they are created using a neural network which undergoes back propagation by predicting masked words. Thank you. Thank you for giving me your time. Thank you for coming on a Saturday and I'm sorry for some of the technical issues that we had. Next time what I'll try to do is I'll try to do it in a way that there are no technical issues. This time we had some internet connection problems. I'm sorry for that. If you have any questions, let me know in the, in the comments of this video. And if you liked it, then please hit the like button. I'll see you next Saturday at the same time on a next, the next session will go deeper into transformers, maybe the feed forward neural network or maybe multi head attention. You can stay tuned on social media. I'll be publishing that lesson soon. Right. So the, the agenda of that. Thank you, infinity. Thank you, Vivek, you were and Karthik, Nikonj, Seth, all of you have been very, very active. Thank you for sharing your questions and also participating. Amit. Thank you. Shikha.",
  "transcript_length": 19469,
  "synthesis": {
    "raw_text": "**Core Thesis**\n\nThe core thesis of this transcript is that vector encodings, particularly those generated by transformers like BERT, are a powerful way to represent complex data in a compact and meaningful way. By training neural networks on large datasets, these vectors can capture subtle relationships between words, phrases, and concepts.\n\n**Key Insights**\n\n* Vector encodings can be thought of as compressed representations of concepts, where each dimension of the vector corresponds to a specific meaning or feature.\n* The process of creating vectors involves tokenization (breaking down text into individual tokens), which is often done using byte pair encoding (BPE) algorithms that merge frequently occurring character pairs into new tokens.\n* Vectors are generated through a neural network that undergoes back propagation, where the network learns to predict masked words in a given sentence by adjusting its weights and biases.\n\n**Actionable Takeaways**\n\n* To create effective vector encodings, it's essential to understand how they're constructed using tokens and neural networks.\n* By training on large datasets, you can develop vectors that capture nuanced relationships between concepts and improve the accuracy of downstream tasks like natural language processing (NLP) and machine learning models.\n* Masked language modeling, a technique used in BERT-like architectures, is a powerful way to fine-tune vector embeddings by predicting missing words in a sentence.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 19469,
    "synthesis_length": 1480,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 1087.584353,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}