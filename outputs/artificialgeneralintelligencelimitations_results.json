{
  "status": "success",
  "media_file": "https://youtu.be/3yEQaHvQxlE?si=6HFqCV7B1-WY7wI9",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_02czu5gp/Why_AGI_is_pure_fantasy_youtube.wav",
  "transcript": "Hi everyone, this is GKCS. This video is a broad overview of the AI engineering space in 2025. Specifically, there are two major things happening right now. One is that we are trying to improve the existing technology. Large language models are being improved in various ways and the other is we are trying to solve problems, which large language models cannot solve. Fundamentally, we'll have to change the architectures. By the end of this video, you'll know what technologies currently exist in the AI space, what are their drawbacks and how we plan to improve on them eventually till we have a timeline of AGI. Currently, most companies are fascinated with large language models. This is where most of the research is happening when it comes to application development. So, GPT-5 is slightly better than GPT-4.5, which was much better than GPT-4, which was far better than GPT-3.5. Dollar for dollar, training GPT-5 is more expensive than training GPT-4.5. The performance boost here is smaller. The performance boost here from 4 to 4.5 is larger. So, you would expect that when you go from GPT-5 to GPT-6, the performance boost is going to be even smaller than this. But it turns out that we have a new type of model, a new architecture. It's a large diffusion model. You'll see that stable diffusion is currently being used by OpenAI to generate images and videos. Gemini is doing the same thing. Gemini is also trying to use diffusion for coding. Another interesting thing about diffusion models is they can train much better on the same amount of data than large language models. And so, because you have the machine returns here, it makes sense to switch your approach, to switch your architecture to a diffusion-based model. This is great, but it doesn't solve one core problem that all of these models have. Take the example where we flip a coin 10 times. And every time we get the result of heads. If we are asked to predict what the next flip is going to result in, any reasonable person is going to first check the coin and then make a prediction that it could be 50% heads or 50% tails. But when it comes to a large language model, it only looks at the previous tokens and calculates an expected output. So for rarely occurring or hard to predict tasks, large language models are really, really bad. They have no internal concept of time, no internal concept of space, and no way to predict the consequences of their actions in the real world. Large language models today can't do this. Neither can diffusion-based models. But if you had an internal world representation or a world model, then you could, using physics or using some of the constraints of the real world, predict what's going to happen when you do an action. For example, if I have the model of a tomato and if I perform the action of throw, then I know that it's going to be damaged. If it's an unripe tomato, maybe the damage is small. If it's a ripe tomato, the damage is large. If it's somewhere in between, then the damage is significant. But what I'm doing now is I'm not trying to predict the next token or the next action to do. I'm actually finding a representation of the world, the world that I'm living in, and I'm trying to predict what actions are going to cause what results. So the large language model is able to filter out actions which don't help in this cause. It's able to focus more on the actions which help bring it to its end goal. And currently, the research by Toyota around large behavioral models, whether using robots to do everyday tasks, is quite promising. There's also Jan Lekhun with the JEPA architecture, which builds world models and tries to solve these problems. However, these don't solve the problem of continuous learning. The problem with any of the models that I spoke about earlier is that once you give them data, once you give them information, they try to do their best. They try to solve the problem on the spot. But if you have trained them on chess or if you have trained them on tomato sorting, you can't take the knowledge from tomato sorting and move the bot to play chess. If you try to do that, then the information or the things that it learned while sorting tomatoes is going to be lost. The internal weights are going to be changed and you're going to be having a bot which is neither good at tomato sorting nor good at playing chess. While humans don't have this problem, they're able to compartmentalize or somehow gain some general intelligence such that they can perform well on both tasks. Humans dream. So at the end of the day, they sleep and they lose a lot of the useless information that they don't need. The neural networks are kind of reset or to be more precise, they retain the information which they got from this day, which is useful for the future. The remaining information is just dumped or put into a backlog. That's what humans do, but we don't know how to do this with a model. Not every biological mechanism can be exactly replicated in a model. Very often we are trying to find the competency and then find a mechanism for that competency. The biological way in which a human does this is extremely complex. For us to even recognize images, there's a lot of stuff which goes on in our brain, extremely efficient. But you can mimic that same competency with something from the metal world. Most of the top researchers like Satin and Lekun are hoping that this is solved. The loss of plasticity is resolved soon, but nothing exists for this. This is the first challenge in the AI space which is currently beyond the research side. The hope again is that by 2030, there will be some sort of research breakthrough where we can find models or we can find the right architecture which is going to have continuous learning. This is a hope of a hope and now we are going to go into completely fuzzy land. Things which have not been solved are nowhere near being solved and which are in the path of AGI. The first hurdle after learning how to continuously learn is system 2 thinking. This was made really popular by a book by Daniel Kahneman. The main idea was that two major ways of thinking, two major modes of thinking were identified system 1 and system 2. System 1 is fast, it's very very efficient for most of the things that you do in your daily job. This is good enough. System 2 thinking is more involved, it's more accurate, requires more effort. System 2 thinking is something that models currently are completely incapable of doing. This includes the ability to have meta thinking, which means thinking about thinking, thinking about things that you don't know, thinking about things that you might learn later. The models are nowhere near this. There are other things also that system 2 thinking has which is when we talk about context window, the things that you have in your working memory. System 2 thinking can pull things into working memory and push them out as and when needed. So if you are seeing five players playing basketball game and a gorilla comes in between, you can actively ignore the gorilla. Crazy. How do you choose to ignore that subconsciously is just something that models are not yet able to do. I said that by 2030 there's a hope that we are going to have this solved. I don't see this happening before 2040, but even if you are believing short time and times, then 2035 is where I can see this happening. Okay, after this comes self-defined goals. And self-defined goals means that an AI system can create its own objectives. Instead of following goals explicitly made by humans, we need the AI to choose one path amongst an infinite set of paths. The problem with self-defined goals is the model needs to have some motivation. So it needs to have a motive and this is not exactly the same as code. Okay, this is what you're supposed to do. When it comes to humans, there's Richard Dawkins who wrote the book, The Selfish Gene, which means that we are all trying to preserve our gene. The gene is trying to preserve itself and we are basically just mediums by which it's keeping its information intact. When it comes to models, there is no such inherent reason to exist. You can try to code that in, but we don't really know how to code that in very well. In the case of an AI model, if we give it the motive of being good, doing the maximum good for humanity, then the model needs to understand what should I pursue, what should I explore. And that always comes internally, inherently in humans. It's not very obvious how we are going to code this exploratory phase into a model. This is actually even harder than system 2 thinking. Okay, lot of organisms have self-defined goals. They don't have system 2 thinking. This is more essential to the concept of life than the previous things that we have done. Okay, but let's be really optimistic. Let's say by 2040, we will have solved this problem also. Very, very optimistic. And finally, we have governance. I don't know when we are going to solve this. I don't know if I have to predict, but if I'm forced to predict on the fastest timeline, which is possible, then AI governance for humans can come up by 2045. So by the time this implementation reaches the world, 2050 would be the shortest timeline by which we hit AGI. Okay, so in the next 25 years, whenever someone talks about governance and big things like this taking over humanity, usually they are completely wrong. There's a lot of optimism that I've kept here, like too much optimism. We probably shouldn't have so much optimism when it comes to developing technology because there is fatigue in the research space around one thing. Thank you for watching this video. I'll see you next time. Bye-bye. Okay, a short rant here. I recently saw a video of Richard Sutton being interviewed by Dwarkeesh Patel. And I would say, Dwarkeesh, if you're watching this video, it's very nice that you bought such a nice guest to your podcast, but it would be even better if you could prepare questions beforehand by reading some of the papers in this space or just looking at the fundamentals of how large language models work. This is also something I saw in the diary of a CEO, I think, with Joffrey Hinton. And the questions there are so many things that they've already answered many times in their research. And what ends up happening is they are left with giving more clarifications and ending up arguing with the host than actually do some sort of exploration, which is what I would like to see in the future. Some of the questions are so bad that people listening to them might think that this is something to even consider. Like one of the questions was, don't babies mimic their moms in the first 6 to 10 months? And no, they don't. I mean, if you have a baby, you'll know this by default, but if you go through basic child psychology, you'll know that this is not true. So asking such a question then becomes a waste of time. Another thing that I see with AI philosophers in this space, Prashant Bhushan, for example, is a very famous lawyer. I'm a huge fan, sir, if you're watching this. But the idea of AI is going to come in the next 4 years, cause a revolution and we don't need to fight for our rights is not true. You know, there's a lot of things which are there, which need to happen before that can occur. And there is no reason to believe that AI is going to be very nice to us. When we can't even understand ourselves, despite having all the time in the world, how do you expect us to express ourselves to systems through code? Our intent, our understanding is probably going to be lost in translation. All the problems of humanity are going to exist for many, many decades moving forward. They can't be solved by AI. That looks more of a coping mechanism to me. Okay, all right. I have now sufficiently angered everyone, but I just want to say to the people who I mentioned here, Duaakesh Patel or Dariy of a CEO or Prashant Bhushan Ji, I'm actually very, very happy to see you in this space. And I just want you guys to maybe look more into it so that we can have awesome discussions moving forward. Thank you. See you.",
  "transcript_length": 12140,
  "synthesis": {
    "raw_text": "**Core Thesis**\n\nThe current state of AI engineering, particularly in the area of large language models, is limited by its inability to solve complex problems, lack of internal concept of time and space, and inability to learn continuously. The speaker suggests that a shift towards new architectures, such as diffusion-based models, and the development of system 2 thinking, self-defined goals, and governance are necessary for true Artificial General Intelligence (AGI) to be achieved.\n\n**Key Insights**\n\n* Large language models have limitations in predicting complex outcomes and understanding consequences of actions, making them unsuitable for tasks that require human-like intelligence.\n* The current trend of improvement in large language models is plateauing, with diminishing returns on investment, suggesting the need for new architectures and approaches.\n* System 2 thinking, self-defined goals, and governance are essential components of AGI that are still in their infancy and require significant research breakthroughs to achieve.\n* Continuous learning and the ability to compartmentalize knowledge are key aspects of human intelligence that current AI models struggle to replicate.\n\n**Actionable Takeaways**\n\n* Invest in researching new architectures, such as diffusion-based models, that can overcome the limitations of large language models.\n* Prioritize the development of system 2 thinking, self-defined goals, and governance in AI systems to achieve true AGI.\n* Encourage interdisciplinary collaboration between researchers, policymakers, and industry leaders to address the challenges and complexities involved in developing AGI.\n* Recognize that achieving AGI is a long-term goal that may take decades, rather than years, and requires patience, persistence, and significant investment in research and development.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 12140,
    "synthesis_length": 1835,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 97.254585,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}