{
  "status": "success",
  "media_file": "https://youtu.be/Z3uWleYwOQA?si=c7dhws5GPYvps0Dp",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_e_fb9ajx/How_I_built_an_AI_Teacher_with_Vector_Databases_an_youtube.wav",
  "transcript": "Hi everyone, this is GKCS. In this video, I want to share with you how I built an AI teacher using vector databases and chat GPT. By the end of this video, we'll know what vector databases are, the internal workings of a vector database, and what are some good solutions in the market. The reason I built this bot is because I have a startup called InterviewReady. It has video lessons, it has PDFs that you can download, and often students have questions which are related to the lesson. What would be ideal is if the user, while logged in, would get a response immediately. Now, there are two ways to do this. One is what a lot of people do, which is hiring, teaching assistants. This of course is expensive, not just in terms of paying salaries, but also in terms of training people, getting their responses up to the mark. So, the human element is a bit erratic, expensive, and slow. If you can't afford it, or if you don't have the people to train them, or if you want to get very fast responses, there's option number two, using a large language model to immediately respond to queries. All the problems here are taken away, but the quality of the response has to be ensured. How do you ensure that with a bot? The first idea was very simple. We use the plain APIs from OpenAI. So, whenever a user asks a question, we forward it to OpenAI and return the response. So, we went ahead and bought a license for charge. I added $100 in the API, and then we tested, but the responses were very bad. That's when we read up about something new, something interesting, which is vector databases. The basic idea here is that you have a video file on load balancing.mp4. You take the transcript of this file, and then you store this text file. You send this as a request to charge gpt. You say, please store this file. When a user sends a doubt for a load balancing question, we are going to say, listen, go to the load balancing transcript, read that file, and use it to give a good response. Remember, we are being charged for tokens, the number of characters we are sending. We are not being charged for how much we are storing inside. I mean, this is going to be a one-time cost. Now, if you just use the single file, like load balancing, that's just going to be one video, and one video is quite small. It's just five minutes long. Also, another problem is it's just one video. So, there could be doubts around that area. Any auxiliary doubts, any doubts which are related to that concept will just be dropped out. So, this is not the best approach. What we should instead do is look at the videos which are related to load balancing. All videos which are related to load balancing should be used to gain context by charge gpt, and then send a response. So, if I knew that video number one, six, and eight are related to the load balancing video, which is video number 10. If I could say that these are similar to it, then I could tell charge gpt to use all four files of one, six, eight, and 10 to give a response. The quality of that response will be much better. It will almost be human-like. The technology that we are looking at then is vector databases. Vector databases can answer queries like, given an object, find all similar objects. If you send an object, in this case, a transcript to a vector database, it processes the transcript and then represents that object using a point. For example, let's say you're watching this in 2D right now. So, a point in 2D can be represented using x and y. If I have a video transcript, one thing I can use the x-axis for is the length of the video, and the y-axis I'll use for frequency of the term system design. If I want to add a third dimension, I would either need to start showing you 3D video, or I can use colors, which is frequency of the term low-level design. Blue means high frequency, and red means low frequency. Now, if I get a query for low-level design, which points do you think will be relevant for me? The ones having blue, right, high frequency of low-level design, that's maybe something I should be targeting. Also, maybe the length of the video is not too large, so the density of the term low-level design is high. That could be something I like. Similarly for system design, similarly for load balancing, similarly for different topics. Now, three dimensions is too small when it comes to making a point like this, because you have various terms that you want to represent in this space. You probably look for an n-dimensional space. This point is easily represented in a vector database. So, you don't have to do any kind of cartesian calculations. You don't have to do a cosine. All of that is provided for you using the APIs of a vector database. And a really good one that I found is actually Neon. There are many reasons I went for this. I am using Postgrease at interview ready. So, Postgrease is really nice. And there is a vector database called PGvector. This is what I came across. If I had to host this, I would have to manually host PGvector on my AWS instance. So, this would be a bit of a challenge. Instead, Neon actually gave a large number of credits. This database also has good documentation. So, I ended up using it. So, when a user actually has a query, when they ask a doubt under a video, the server sends the query and the content piece to our vector database. This tells you all similar points, which is basically similar files, similar transcripts, similar videos to this query. Now, we take these similar files, take the names of them or the IDs of them, and we send this to chat GPT. We say, answer this query keeping in mind that these are the files you should look into. Chat GPT gives us a response, and then this response is forwarded to the user. Let's now look at how this is actually implemented. So, the first thing I need to do is take the videos and the PDF files that I have and convert them into text, text information. So, here I have used AWS Transcribe to do this. It's a very cheap service. It's reasonably good. Eventually, as a creator, you always have to go through the transcription once anyway. So, cheap is the only factor that I'm looking for, and AWS is pretty good. You can also get it for free in Adobe. So, that's even better. But I also have live classes on Zoom, so I want a single place to do this. Now, I get a file at the end of this, which is a result, and I go to Neon Serverless. Neon, after all, is Postgres with the wrappers that I need. I'm sure there's a lot of other solutions also out there, but their code is open source, so I can literally go and check the things that they're doing. For example, in a drag model, one of the things that you want to do is, how is the vector database working internally, in case you want to know. So, they use this algorithm, which is hierarchical NSW, which stands for Navigable Small World. The benefit of this is that when you have a bunch of vectors in the spatial domain, so n-dimensional vectors, you want to cluster them all together, the ones which are close to each other, and those clusters will have representative vectors. So, if you have, let's say, 5000 vectors, and each cluster is of size 100, then you'll be left with 50 clusters, and each cluster will have one representative vector. The benefit of this is instead of comparing between 5000 vectors, you're now just comparing with 50 vectors. And one very useful benefit that I saw here is that you're actually able to see the history of your changes on Neon. You can walk back, step by step in history, and go forward also, and things will be fine. Why am I doing this? Because I've heard when you're creating vector databases, when you're having these kind of search queries, sometimes you want to know what you did in the past. Whenever you're running an AI model, it's not just that your code changes need to be committed, like in Git, you have these commits where you can see what code change happened. But in reality, when you're running an AI model, your data is also changing. So, for the same code and same data, you should get the same response. But tomorrow, if you want to see the performance of this model versus some other model, the data also has to be the same. So, my database has to have some sort of versioning history. Neon provides that with a walkable interface. So, overall, I really like this database. That's the reason I'm putting it forward so strongly. Also, I really like AWS in general. And if you're looking for how did I upload these files, these transcription files to chat GPT, just Google the API reference for OpenAI. Here, you will see various API endpoints, the one that you're looking for is files. So, you will be able to upload a file into OpenAI. And what you have to do is you have to just mention a few parameters. This is very easy. Once the file has been uploaded, you can use this file later on to answer queries. So, later when you want to have a conversation with the AI, you can do that. In fact, let me just show that to you. Here, I have given a general instruction. You are a system design tutor to the AI assistant. I am also saying the files that you have to use are 175, 176, and 177. Now, this is going to be fetched from Neon as a similarity search. And the query is what is load balancing? So, let's see what happens. Boom. Load balancing is a process that distributes network and application traffic across a number of servers. This helps to increase the availability and the reliability of applications or websites by ensuring no single server bears too much load. That's it. A simple answer, which has been derived from the files that we have sent beforehand to chat GPD. And the files have been chosen by Neon DV because you know that these files are the ones which are most relevant to the query. With this, users don't have to wait for 24 hours to get an answer on their query. They get an AI generated answer, which may be suboptimal, but at least it is a response. Later on, an admin can come and look at the answer, look at its quality, and make a new reply or delete the old answer and make a new reply if they want to. This is called retrieval augmented generation, R-A-G. This is becoming very popular now in the AI world. The basic idea is that you retrieve context from vector databases that store similar content. Then you can augment this query with context. The final step is generation. Here, I use chat GPD because it's got a very easy API. I understand it quite well. I manually test queries also on it. So in many ways, this was the easiest to use, but I've heard Lama is also very, very light, very, very good. And there's other large language models also, of course, which you can use. Thank you so much for watching this. If you have any doubts or suggestions, you can leave them in the comments below. I'll see you next time. Bye-bye. I wonder why you're sticking around. I just said bye. But if you're looking for implementation details, I'll give you some hints. Now, AWS and OpenAI are pretty well documented. I don't think you'll need help there. But for creating a vector database, what do we need to do? Let's log in to this portal. Just click on new project. You can add a name here. I'll just say YouTube project. And depending on your salary, you can set up the instance you want. Also, depending on your region, you can set up the instance I usually pick up Singapore. And then I create the project. Now, you see that there is this branching which neon offers. And it's got a bunch of these connection strings which can be used. Because I'm using Go in the backend, I will go for this. And I can just copy paste this code. It's literally that easy to connect to this database. You might need to change it a little bit because you won't have a main function, hopefully, which is picking the database URL. But the good news is all the libraries which are necessary, all the code for writing the connection string is already here. If you're looking to play around with the DB, you just go to tables over here. And you'll see all the tables you create right here. Okay, you can file SQL if you like, have an integration with GitHub, lots of stuff. But I think frankly speaking, the most important part is just creating the connection string and actually being able to use this DB. It's ease of use is pretty good. So all the best. See you soon.",
  "transcript_length": 12380,
  "synthesis": {
    "raw_text": "**Core Thesis:**\nThe speaker, GKCS, shares how they built an AI teacher using vector databases and chat GPT to provide immediate responses to user queries, eliminating the need for human teaching assistants and ensuring high-quality responses.\n\n**Key Insights:**\n\n* Vector databases can answer queries like \"given an object, find all similar objects\" by representing each object as a point in n-dimensional space.\n* Using a vector database with a large language model (like chat GPT) enables Retrieval-Augmented Generation (R-A-G), where context is retrieved from the database and used to augment the query before generating a response.\n* The speaker uses Neon, a vector database service, which provides features like hierarchical NSW algorithm for clustering vectors and versioning history with walkable interface.\n* Vector databases can significantly reduce the number of comparisons needed between objects, making them efficient for large datasets.\n\n**Actionable Takeaways:**\n\n* To build an AI teacher using vector databases and chat GPT, start by converting your video lessons and PDFs into text using a transcription service like AWS Transcribe.\n* Use a vector database service like Neon to store the transcribed text and enable similarity searches.\n* Integrate the vector database with a large language model (like chat GPT) to enable Retrieval-Augmented Generation (R-A-G).\n* Consider using services like OpenAI's file upload API to upload transcription files for later use in conversational AI.\n* When implementing a vector database, focus on creating a connection string and using the DB's ease of use features, such as table management and SQL integration.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 12380,
    "synthesis_length": 1666,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 202.5899,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}