{
  "status": "success",
  "media_file": "https://youtu.be/n7xsHhmlMEQ?si=H1QD6ndoolDyXBCE",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_e9j6pj01/AI_Engineering_4__Reasoning_models_youtube.wav",
  "transcript": "Zoom in. This is number one, this would be number two, this would be number three and today what we are going to look at is here, number four. And I would like you to think of an example where a model is expected to think, thinking LMS, reasoning models O1, O3, this type. Let's take a question, how many sides does a square have? This is the question and I am going to give you two answers, answer one and answer two. Let me know which one you prefer. Answer one is four, answer two is it depends on your definition of square. Which of these two answers do you prefer? Right, the vast majority of people will actually prefer answer number one, which is four. A direct answer actually gives the answer. This is a roundabout way of saying that I don't know and there are other ways also. I mean you can literally also say answer number three, I don't know and answer number four can be that's an interesting question. So by far we would prefer something which solves the problem. There is one challenge here though, when you look at any large language model, it's trained on input tokens and predicting output tokens. So if I look at this statement, how many sides does a square have? In the mind of the large language model, both of these answers are perfectly sensible. They are grammatically correct, the language is perfect. There's nothing wrong with the second answer in the mind of the large language model. Okay, because what it's doing is it's predicting tokens and so if the probability of the tokens is reasonable, then the answer is also reasonable. Instead of four, it could have also been it is equal to four. This would be an equally high quality response as A1. So A3 and A1 are equally good. But there is no way for the large language model to be able to say this. Okay, all it does is it finds the probability of the next token that it should spit out. What would be a technique to improve on the chances of getting the answer A1, including the probability of A1, as compared to the probability of A2? That's what we are going to go through in this class. And so we need a starting point guys. Can you help me by thinking about any possible solution? Given a large language model which comes up with good output probabilities, which tells you what should be the next word, how would you train this model to actually start responding to mathematical or geometrical questions in a reasonable way? This model itself is not complete and Prakhar has the right answer. Treating more math context to the model. So what happens is initially you have this base model, you have this series of transformers which have gone through tons and tons of text. Let's say one trillion tokens. That's how much you're trained this model on. And what does it do? It can tell you with very high precision what the output probabilities should be for any given statement. Unfortunately, it does not make sense. It does not have a brain. So it can't tell you whether it should respond for or it depends on your definition of square. But it can tell you what the output probabilities are like. So it can construct sentences and it understands the grammar and sentence composition of the language you have fed it. In our case English. So at this point, at step one, if this is step one, we know how to construct sentences. In step two,",
  "transcript_length": 3324,
  "synthesis": {
    "raw_text": "Here's the structured response:\n\n**Core Thesis**\n\nThe core thesis is that large language models struggle to provide direct answers to mathematical and geometrical questions, instead often providing ambiguous or vague responses due to their training methodology on input tokens and predicting output tokens. To overcome this limitation, the model needs to be trained in a way that incorporates more context specific to math and geometry.\n\n**Key Insights**\n\n* Large language models are primarily designed to predict output tokens based on input tokens, rather than providing direct answers to questions.\n* The model's response is determined by calculating probabilities of subsequent tokens, which can lead to ambiguous or vague responses, especially in mathematical and geometrical contexts.\n* Despite grammatically correct and reasonable-sounding responses, the model may not be able to provide a clear answer to a question, such as \"how many sides does a square have?\"\n* The model's inability to distinguish between direct answers (e.g. 4) and ambiguous responses (e.g. \"it depends on your definition of square\") is due to its design and training methodology.\n\n**Actionable Takeaways**\n\n* To improve the model's performance in mathematical and geometrical contexts, it needs to be trained with more context-specific input data.\n* One possible solution is to incorporate additional math-related context into the model through techniques such as data augmentation or curriculum learning.\n* The model can benefit from being explicitly trained on mathematical and geometrical questions, rather than relying solely on its ability to predict output tokens based on input tokens.\n* By incorporating more context-specific knowledge, the model can learn to provide direct answers to mathematical and geometrical questions, rather than ambiguous responses.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 3324,
    "synthesis_length": 1847,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 435.15638,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}