{
  "status": "success",
  "media_file": "https://youtu.be/uBL0siiliGo?si=GgG25YuzrgnCRbTj",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_86kpj70s/Model_Context_Protocol_A_Deep_Dive_into_the_future_youtube.wav",
  "transcript": "Hi folks, this is GKCS and we are talking about the model context protocol now. This is something which has tremendous potential, but I think because there is so much hype in the AI space right now that people have completely missed the potential of this new protocol. The current problem with large-language models is that they have some intelligence, they can tell you what you should do, but they can't do the thing for you. They can't take actions, they can only give outputs in text or other mediums. Let's take an example. You are a software engineer and you have a production outage. Now what you want to do is pick up all the requests which have been coming to this server, run the request locally, look at where you have the problem, make the code changes for it and then deploy this new branch to main. If this tedious process could be taken up by a large-language model, then you would require lesser engineers to do the same jobs and you could send the engineers to Goa instead of having them on call. So with this basic idea that you want your large-language model to not only give you advice or give you the steps that you need to follow, but also actually do things for you comes the model context protocol MCP. This enables a two-party system where you have clients who are large-language models and you have servers who are typical servers, exposing APIs. The client can send a request to the server to do something. This isn't a new concept. The unique thing about this is that you have some sort of intelligence here. A large-language model can decide what to do based on the current situation. There was IF-TRIPLE-T which used to allow you to take events from external systems and react to them. But LLMs are smarter than just if-else conditions. They can take the decision required in the given situation. Okay, so that's the most basic use case for MCP. What are the use cases can you see? Three major ones. The first one is search engine optimization. This is where Google ranks your page based on how relevant it finds it for an incoming query. So if someone searches for system design course and if interview ready comes on top, then my startup benefits because the chance of people actually purchasing the course just increases because it's more searchable. It's more discoverable. So now the traditional search engine optimization which tries to rank you on top is no longer sufficient. You're looking for a slightly different approach where your website should be linked to create the consolidated result. How do you increase the chances of this? Well, if you have MCP servers which are exposing APIs, the chance of your website coming in the final result is quite high. I'll take an example. Let's say you want to get the top 100 ranked coders on code forces. You could do this by scraping pages, but it would be much better, much faster, much more structured if you had an explicit API available on an MCP server which could be accessed by a large language model. Now the large language model hits that API, gets that response in a structured way, uses it to create the final response. And so we may see a major change in how SEO is done across websites. Earlier you would look at light speed, you would look at how quickly a response came, how quickly your webpage loaded, how accessible it was. With large language models, your page may not be accessed by a human. It might be accessed by a LLM which doesn't really care so much about the look and feel or how easy it is to read. As long as the information is there, it's good. And maybe you're looking for more information than is usually readable by a human being. So things are still very, very new. The field is being called LMO which stands for Language Model Optimization. Everything which is considered good search engine optimization principle is also being applied here. There are two other major use cases for MCP servers. The first one is retrieval augmented generation. This has been in AI hype for the last six months and it has lived up to its hype because now when you ask chat GPT for the latest information, it doesn't say, oh, I haven't been updated up to that point so I can't answer that query. No. Searches online, finds relevant data sources, fetches them, converts that data into vectors and uses them to answer your query in the best way possible. It could also be using local data sources which are updated at a different frequency, let's say once every hour versus the model which is updated once every six months. So what you have done is separated the frequency of data updates to the frequency of model updates. This really helps because model training is very expensive while fetching data is super cheap. So retrieval augmented generation gets better through MCP because now the external sources can answer your queries in a better format and also much faster. And the final major use case I see for MCP servers is applications. An example of this is a user asking to rent a car. This has always been a difficult use case for search engines. If you list these out, then the user has to individually search these. How can large knowledge models do better? The aggregator websites take all of the top search results and try to condense them into one place where a user can compare different cars, compare the cost, compare the size and everything else and then make a decision. So now the aggregator result has to turn into an aggregator MCP API. If you're looking to find the best car to rent, the MCP server which you have created in your application can go and fetch results from different websites, aggregate it into a single API and give that response to the LM. The benefit for the LM is that this response is much faster, it's better consolidated and the benefit for you is either the LM paying you for using your API or the websites who you're scraping paying you to rank themselves higher in the result. It's almost like you can charge these websites to put up their ads in your MCP server response which is eventually going to be consumed by an LM to give the final output. Now this hasn't happened yet. The monetization part of MCP servers is not very clear and the ecosystem is very young but I do predict this change coming in the future where large language models, even if they don't explicitly show you ads, may be forced to show you things which other people have paid for because they are hyped up, the content is boosted before an LM can parse that content. So that's major lead for model context protocol. It's got a lot of potential but whether it will be realized or not depends on the large companies right now. Google, OpenAI can decide to accept this as a way to interact with different servers or they can just ignore it completely or they can hack this protocol and make their own versions out of it like they did with Android. Thank you for watching this video. If you have any doubts or suggestions, you can let me know in the comments below. If you liked the video, then hit the like button and I'll see you next time. Bye bye. One small thing around the future of these kind of protocols. I don't mean to say MCP is going to make it big but I can see one major improvement over the protocol which would really help make this much more useful. The main benefit of MCP is that it lets you perform actions but it would be really nice if the actions could be more capable. In the sense if I have to send an email without Gmail access, I can't send an email. So what if Gmail had an MCP server? If it did, it would need authorization from my side before any emails are sent by Chargivity using the MCP server because it's my email address. I have to say yes before any email is sent. Now there already exists something around this. It's called OAuth where when you're logging into a website and you don't want to add your email and password again, you just go and sign in with Google. What Google does is it hijacks the page and asks you permission to share your email address with this new server. If you say yes, then the server gets your email address signed by Google. So it knows that you are an actual user with this email address and it can just log you in. But the potential is much bigger. At interview ready, we take calendar permissions for any kind of future Zoom classes. If these two protocols come together, then you can have tremendous capability in the MCP server backed by tremendous permissions through the website which we often use. This is where I see the future for large-dengaged models. It's very similar to agents, kind of like personal agents, but building personal agents per person is really hard. If the larger companies pick up MCP servers or some sort of capability servers that large-dengaged models can use, then you'll see the amount of human work reduced dramatically. And I hope that it happens. I hope the mundane things that we do in life reduce a little bit. In fact, I am going to create an MCP server for interview ready just for learning purposes and understanding how this works. I'll make it open source so that all of us can contribute and learn along the way. If you want to share it on your LinkedIn or resume, please go ahead. Feel free. I'll link the GitHub repo in the description below. Thank you for watching this and I'll see you next time. Bye-bye. Bye.",
  "transcript_length": 9345,
  "synthesis": {
    "raw_text": "**Core Thesis**\n\nThe Model Context Protocol (MCP) has tremendous potential to revolutionize the way large-language models interact with external systems, enabling them to take actions and perform tasks, rather than just providing text-based outputs. This protocol has the potential to significantly reduce human work and automate mundane tasks.\n\n**Key Insights**\n\n* MCP enables a two-party system where clients (large-language models) can send requests to servers (typical servers exposing APIs) to perform actions.\n* The unique aspect of MCP is that large-language models can take decisions based on the current situation, unlike traditional if-else conditions.\n* Three major use cases for MCP are:\n\t+ Search Engine Optimization (SEO): aggregating results and providing a consolidated response to improve search rankings.\n\t+ Retrieval Augmented Generation: using external data sources to answer queries in real-time, rather than relying on model updates.\n\t+ Applications: integrating with aggregator websites to provide faster and better-structured responses to user queries.\n\n**Actionable Takeaways**\n\n* Consider implementing MCP servers in your applications to enable large-language models to interact with external systems and perform actions.\n* Identify opportunities for using MCP in search engine optimization (SEO) to improve search rankings and visibility.\n* Explore the potential of retrieval augmented generation to provide faster and better-structured responses to user queries.\n* Be prepared for the possibility that large companies like Google or OpenAI may adopt MCP, leading to a shift in how large-language models interact with external systems.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 9345,
    "synthesis_length": 1662,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 165.761136,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}