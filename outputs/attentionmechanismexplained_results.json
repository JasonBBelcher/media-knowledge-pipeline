{
  "status": "success",
  "media_file": "https://www.youtube.com/live/jPGaYb853GM?si=1rFilKM7uIO-TzSr",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_4fxn0b6i/AI_Engineering_1_Attention_is_All_You_Need_youtube.wav",
  "transcript": "If you are a software engineer looking to transition to AI projects or an AI role, it makes sense to understand how transformers work at a deep level. You need to know how transformers work and for that you need to know how attention works. So that's the mental mind map that we have and the goal of this session is to help you understand how you can start working on AI projects and the way you will get confidence for that is by knowing how things work internally. Let me start with a simple example. Let's assume that you have a query for a large language model and this model gives you an output, very standard stuff. You can think of this as chat GPT. Now I'll give you an example of the query. The query is I went to the bank dash. Now when I say the word bank, what do you think of? Most of you must be thinking of a financial institution where you have all your savings. But bank can also mean river. So if you are a normal person or a banker or if you are related to finance, most likely you thought of a financial institution but if you are into clearing rivers, if you are major in geography, you may have thought of river. So the current thing that we have to remember is there are different words and some words have multiple meanings. So firstly before we get to the depth of this, I went to the bank. Can anyone help me complete this sentence? In the output what can I expect to get? I went to the bank. Next word could be to, next word could be make, next word could be a, next word could be deposit. We went to the blood bank, a gene bank. Yeah, nickel that is also a very interesting way to think. So it's no longer a financial institution, it's actually a storage facility. So banking means something but just bank might be just the storage facility. I went to the bank today. So that's a good example. So example number one, possible output. Example number two, possible output. I went to the bank, dash today and Saurabh has come up with this statement. The interesting part is the stop character happens right here. While I could have said to make a deposit also. I went to the bank yesterday. Various examples exist. How is the large language model actually coming up with this? What's going on internally? Let's see. We have an input statement. We then look at every word of this statement. For now assume that it's word wise. It's something called tokens. So we'll write down those tokens. For us the tokens are space separated words. I went to the bank. And what I'll do is note down all the characteristics of these words. So let me note these down. What kind of characteristics am I talking about? Is it a place? Is it related to finance? And for now let's just keep two characteristics. So it's like a typical excel sheet, rows and columns. All of the columns have all of the words which I input and all of the rows have characteristics of the words. So let's see. Place. Am I a place? No. Is it a place? No. Is wanting a place? No. Two has some relation to place. So I'll just make it a 0.5 or a 0.9. You always go to a place. The has no relation to place. Bank is a place. So I'll give it a relation of one. Next I have something like finance. So this is another characteristic of all the words and you can measure this characteristic. So am I related to finance? No. Want might be related to finance. So I want some money. I want something. So I'll just give it a score of 0.2. Low relation. Two, zero, the zero. Bank has high relation with finance one. And similarly we can take more characteristics. What am I doing here? I'm looking at different ways in which I can measure the characteristic of each of these words. Okay. And what's going to happen is at the end of this I'm going to have a unique representation of each word. If I keep increasing the number of characteristics of every word, at the end of it I will have a unique representation for all possible words in the English language. The basic idea is that we want the large language model to generate good output. The only way it can generate useful good output is if it understands the underlying meaning of each of the words that we have sent in as an input. Okay. So to do that we need to describe the words. We need to describe different characteristics of every single word in the English language. Assuming that the model is English. If it's an image model then we would need to describe every single characteristic of every image possible. Saurav is asking, so these are basically weights to every words based on probabilities of specific context. Let me describe this in a more clear way. As you add more and more characteristics for now I will start adding a person. If I add the characteristic of person, I has a characteristic of one. It's a person. One can't say. Two can't say, the can't say, bank is not a person so it will be zero. This is the person characteristic. I can have another characteristic which is computer. Is it something which does computations? I may be 0.5. Want zero, two, two, bank. Bank also does computations so I will keep it at 0.5. What we are trying to do, don't focus on the numbers so much. Focus on the fact that we are taking every single word which is input to our large language model and we are trying to describe it in the best way possible. If you go along the column vector, if you go along the column then you will get values for every row which uniquely describe this word. Why is this important? If you can describe the words, only then can you understand the meaning behind them and only then can you come up with useful output. There's a very interesting question from engineer Baucha which is where are you getting these characteristics from? For a given query, how do you know how much a characteristic is? We will not get into that right now. Now assume that this is happening all magically. We know the characteristic of every single word but it's actually a neural network which is trained and it gives out some sort of a column for every single word. We are not going into vectors right now because vectors will be confusing to think of. I'm starting with simple excel sheet for every word. We have all the characteristics as numerical values. But it's not that humans sit down and figure out all the characteristics that's going to be impossible for all the words in our English language. This happens automatically through a neural network. They generate vectors. These words will have the same numbers attached to them or will it change according to context? That's a very good question Prabhat. As you see they will change. But given an input at this point in the start, you can't tell whether the the bank is a financial institution or a river or a storage facility. So the word bank means the same values. This column bank, I just highlight that in green, this column bank, this vector, a 1D array, this is going to be unique and a one-to-one mapping for the word bank. I have no idea whether you are talking about the financial institution or you are talking about the place or you are talking about the river. I am not a river, want, two is not, but river is also one in bank. And someone had mentioned blood bank, so blood is probably 0.5, storage is probably 0.9. So in this way you are getting a very unique 1D array which can uniquely describe this bank. Each of its indices is actually some sort of characteristic. And if you are able to describe it so well, if you are able to describe every single word uniquely with relevance to the characteristics that it has, then you can assume that the large language model can understand what those words mean and come up with useful output. Its purpose is to come up with useful output. It can only do that if it gets, if it can derive the meaning. Naman is saying there can be infinite possibilities. How and where will we stop? We will get to that eventually. For now just assume that there is a way to stop. There is a way to tell very precisely what every word means. And we send it to this large language model. So we have this description. Let me now go for the next part which is attention. It is much easier than it looks. I mean there is no, the prerequisite for understanding attention is much smaller than most, you know, most of us would think. Take the same sentence, I went to the bank. And again take these in the rows also. I have taken the words in the rows and the columns. It is the n cross n matrix. In our case n is 1, 2, 3, 4, 5. 5 cross 5 matrix. Take all the input words, put them in rows, take all the input words, put them in columns. That is all I have done. This is called the attention matrix. Why? The naming does not matter. But what does it do exactly? Take the first word I and I. Find the relation between them. Very high. Of course it is the same word. So I will give it a score of 1. Take the second word I and went. Okay. Find the relation between them. What do I mean by relation? I mean that take the first word I. That is over here. That means take the entire column of I and take the second word which is went. That is over here. Take the entire column of went and do a dot product on it. Dot product is 0 into 0 plus 0 into 0.2 plus 1 into 0 plus 0.5 into 0 plus 0 into 0 and so on. So you take every row, multiply the values, go to the next row, multiply the values, you do this and then you add all of them together. I will get a result, may be 0.01. Okay. So I is related to went with a relation score of 0.01. I do the same thing with I and 2. So the green vector stays the same. I but 2 is blue or I will make it yellow here. 2, 2. Take the entire column and we are looking at I and 2. So first column 0, second column 0.9. 0, 0, 1, 0 and so on until I do a addition of all of their row multiplications. Okay. 0 into 0.9 is 0, 0 into 0 is 0, 1 into 0 is 0 and so on. In this way, I get another value. Let us say 0.02. Is this making sense? What I am trying to do is I am trying to find the relation of every word to all the other words in the text. Why are we doing this? Go back to the original question. Go back to the original problem. When we say I, it is clear what I mean. Okay. This is easy to understand. Went, it is some sort of movement to a place. It is to a place. The, some sort of article. Bank, big problem. What do you mean by bank? If I say bank, some people think blood bank, some people think financial institution, some people think that it is a river bank. How do I get rid of this ambiguity? By looking at the other words in the sentence, they give me the context on bank. They tell me what I am trying to say. So what I need to do is I need to find the relation that bank has to all of these other words. Somehow try to encapsulate that meaning in bank. And then if I pass this to the large language model, it is going to come up with very good outputs. It is not going to tell me go to the blood bank. It's going to tell me go to the financial institution. Is this making sense? The reason why we are trying to find relations between bank and all the other words, because bank is an ambiguous word, bank needs context. The context can only be derived from the other words in the same sentence or other words in the same input query. Okay. So our original problem was we have an input text. We want to generate the future of that text. The only way we can actually do this, the only way we will be able to come up with meaningful, useful text that people like is if we understand the underlying meaning behind the input. To derive the underlying meaning behind the input, we try to describe it in the best way possible. But there are some ambiguous words. So to get rid of that ambiguity, we need context from the other words in the same sentence. Is this making sense? Now our large language model will have a lot of context, we'll be able to derive the right meaning for the word bank. It won't be some static meaning. It won't be just this vector. It's going to look at the rest of the words and change this vector. It's going to change the meaning. Is this clear? Any questions in our folks? Let's run through the example. Let's run through the example of bank, right? Take bank and take the word I. So this, let me just get rid of that too many lines here. It's going to be very confusing. So I'll just get rid of these things. Yeah. Take bank and first check its relation with I. So maybe I get a value of zero. Now take bank and take its relation with want. Maybe I get a value of zero again. Take the same word bank, relation with two, value of zero. Take the same word the bank and relation with the zero bank and relation with itself is one. So what can we see here? We can see that its relation with the rest of the words is zero and its relation with itself is one. This means there is no added context from the other words to the word bank. Looking at this sentence, you cannot tell. Just looking at the sentence, you cannot tell whether you went to the riverbank or the financial institution or the blood bank. Okay. Is this making sense? Let me add another word so that things get easy. Let me add a few other, a few more words. So I went to the bank.to make a deposit. Let's change this. Make a deposit. Alright and this will also increase the number of columns that we have. Is this making sense firstly? As a human being can you look at this sentence, I went to the bank and confidently tell which bank we are talking about? It's impossible. You can't tell whether it's a blood bank or a financial institution. So we need more context and if you write this in chat GPT, it's going to assume that it's a financial institution because most of the times you are talking about that. But it's not sure. On the other hand, if I add these words to make a deposit. Now again, I have an n cross n matrix. I have all these rows and I have all of these columns. I look at the relation of bank with two, it's zero relation of bank with make zero a zero. What about this deposit? What do you think the relation of the word bank is going to be with deposit? It's going to be non zero. Let's give it a value of 0.5. Okay. How did we find the relation? We took a dot product between the bank 1D array and the deposit 1D array. Assume that the deposit 1D array is here. I have the values. Is it just give me a second here? Yeah. You see, is it a place? No. So I'll give it a value of zero. Is it related to finance? Yes. So one. So one plus one is one. This is where you get the relation. Is it a person? No. Is it some sort of computation? Let's say zero. Is it a river? No. Deposit has nothing to do with the river. Blood. I mean, yeah, you can do blood deposit, but for now assume that these are all zero. But some non zero relation happened because you had one plus one here. When you try to find the meaning behind bank, you saw that deposit adds meaning to that word. How is this helpful? Again, coming back to the original problem, you want to generate words. The only way you can do that is by driving the meaning. Some of the words are ambiguous. To find the contextual meaning, you look at the other words in the same sentence and then you push the word to the right meaning. So bank in our case because deposit has some relation, we will now make the assumption that you are talking about a financial institution. Saurabh had come up with an example, which is I went to the bank today. If I use the word today, again, I can't tell today has no relation to bank, but someone had mentioned blood. If I say I went to the bank today to donate blood, then what's going to happen in this attention matrix is you won't have the word deposit. You will have the word blood in one of the columns and that's going to give you a nonzero value, which is going to tell your large language model that, hey, it's a bank, but please push it to the blood meaning, storage meaning. This is really clever. Just think about the fact that you have an input statement. The statement is broken into words. We try to extract as much meaning as we can out of each word. Unfortunately, word meanings are static. The large language model cannot tell if you are talking about Apple, whether you're talking about a fruit or a company. If you're talking about a crane, are you talking about the machine or the bird? That's impossible to tell just looking at the words. It tries to, it tries its level best to drive as much meaning as it can from the words itself by giving them all characteristics. But some words have multiple meanings in all languages. In Hindi, there's a word called goli, which is basically a bullet or a medicine. And if you're a drug dealer, you think of goli in other ways. So the same word has multiple meanings. And what you do then is look at the context. I can't tell who's typing the input query as chat GPT. I don't know their life history. So I don't have any context. The only context I can get is from the sentence, the words in the same sentence. That's the idea. The large language model looks at the other words in the same sentence adds those words, those relations to the existing ambiguous statement, ambiguous word like bank. And therefore, bank will now become the final thing is going to be this column. Just give me a second. You're going to have the bank column. I'll give it a different color. So let's go. Bank will have this column bank and 0.5 of deposit. You'll have a slightly different vector. The array is going to be changed. Okay. In the other example, it will be bank plus 0.3 blood. These two vectors have different meanings. This is a financial institution. And this is a storage facility for blood. The same word by having some addition, some manipulation can give you multiple meanings. This meaning is essential for the large language model to come up with useful output. Otherwise, it's going to assume that you're talking about bank, the financial institution all the time. It can never imagine that you will be talking about the blood bank. We have to look at the rest of the words in the sentence and give it that context. Is this clear? Is it like prompt engineering to no, this is not prompt engineering prompt engineering is something that you tell the large language model that you know, you're supposed to behave this way. You have a role. And so attention is to do with getting relevant context from the sentence and adding it to every single word. I'll take another example in the same sentence. Look at this. We said make a deposit. Make. The word make for a C plus plus programmer means something. The word make for a potter would mean something. So there are, there's a C plus plus programmer. For them, it's a make file. Make for a potter or an artisan is creating something. And make for us in the case of deposit is to set up something. Okay. They're all three different meanings. What will happen in through attention mechanism is this word make will look at all of the columns, all of the other words in the same sentence. I so value of zero, let's say went zero to the bank. Okay. I'll give it a very small value of point one to make a deposit. Deposit is going to give it the meaning of make. So what does make mean in this context? So I'll give it a score of point three. So make could have been three different things. In our case, we nudged it towards setting up something. If you had told me I made a file so that I could program faster, I would have given you program that word would have sent you towards C plus plus programming. So every single word in your input sentence is going to go through this attention mechanism and get the useful context that you need for the large language model to give a useful output. So she was missing. How is attention scored different from word embeddings? It's not. It's basically the I'm talking about the dot product, which is cosine distance. You can also take Euclidean distance. You're basically trying to find the relevance of one word to another. You can think of this as bank.I think this is the best way to think of it. You can think of the word bank, it's one big word. There is something related to finance, something related to blood and something related to rivers. If the other words, money, here I will say flow, in blood I will say be positive. There are different words which can pull the word bank towards themselves. If you find the words in your sentence, then you start getting a pull. So if you get money or finance, you start moving towards the meaning of financial institution. If you get river or flow, you start moving towards a river bank, meaning. If you get blood or some sort of blood type, then you start moving towards the meaning of blood bank. So depending on the words in your sentence, your original word is pulled towards that meaning. There are three different directions you can go in. The router says go on this side. This has the most weight. There are other words which are pulling you in this direction. If you don't give it the right weight, if you just say I went to the bank today, then there is nowhere to go, it just stays where it is and it is ambiguous. You will make the logical decision of finance is close to the word bank, closer than the rest of these guys. I will make the assumption that it's finance. In general, bank is closer to finance. If only you have river and flow, will you actually pull this word towards them? What we did effectively is take all the characteristics of the word bank, take that vector, that 1D array and add some values from these other vectors to the word bank. It pushed it to the right direction. Now taking this resultant vector, so maybe this is your original vector V1, you took finance, added plus 0.5 V2 to it. Your resultant vector is this, slightly towards the financial institutions. You take this new resultant vector, pass it to the LLM and the LLM says, okay, I know what you are talking about. It's not just a random, like there is no ambiguity now, I know exactly the kind of bank you are talking about. Let me give you a response. Let me tell you how to make a deposit. Let me tell you how you can write a letter to them. Yeah, Rabi Kiren has a question, do the same concepts apply in Hindi? Yes. Japanese, yes. So they not only apply to text, but they also apply to images. So if you see a dog and you see the background is a garden, then you will feel like we are talking about a dog in a playground. But if you see the background is very colorful and completely animated, then this dog is probably animated too. Or a toy dog. If you see in a movie, there's a real dog, but there's a toy who's talking to that dog, then you know that this is a kid's movie. So some context is derived from even images or video. Okay. So these ones? Yes. Such in the corpus of training data is extremely important here. Getting enough data to train this model is a challenge. Getting enough compute power to actually train the model is a big challenge. This is expensive to train. We are just talking about the attention mechanism for now. We are not looking at the scale part yet, but you're right. Yeah. So Ashish has an interesting doubt. He's saying, would the bank pull in the case of I went to the bank? So I went to the bank actually is an ambiguous statement. You only know that you've gone to a place, but you don't know which place you have gone to. So bank is not able to pull the other statements correctly. You have to give more context. For example, if you ask it to chat, if you say that I went to the bank, you tell this to chat GPT, most likely it's going to make an assumption because the vector for bank is going to be close to finance. And so it'll make an assumption that you're talking about a financial institution. But if you say, no, dude, I was talking about the river, then it's going to say, I'm sorry, made a mistake. So yeah, that is a lack of meaning in the sentence itself. Are these attention scores generated when user enters a prompt for large language model, unlike word embeddings, which are already generated in user's reference? You're right. These characteristics that we talked about, these columns, these are pre-generated. You can think of these as pre-processed. The way this happens is by training a neural network to cluster similar words together. So word to WEC is an easy way to think of this. We'll get to tokenization and vectors in a future video. But for now, we have not gone to this because attention doesn't really matter. The concept doesn't have too much to do with vectors. We can understand it even with the Excel sheet. Right folks? So this is the basic high level overview of what attention is. It lets a large language model take words as input. The idea is to generate the best possible output that you can, the most relevant output that you can. For that, you try to derive the maximum meaning that you can from every single word. That is a static embedding. That is a static column for every word, a 1D array. You notice that doing this alone is not going to be good enough. You have to add more context to it because we don't know who the user is. We don't have any context window initially, assume. We are going to take the other words in the same sentence and add that context to ambiguous words like bank, ambiguous words like river. In Hindi, an example of this is goli. Dr. saab ne mujhe goli di means a doctor gave you a goli. It's probably medicine. Police ne usko goli mardi. If police is related to it, then goli is probably a bullet in this context. The same word goli with a static embedding is not going to be generating good output for the large language model. It will have to start making assumptions. We don't want assumptions. We want accuracy, high context. That is derived from the other words in the same sentence. You look at bank and you look at deposit and you say, I know exactly what kind of bank you're talking about. If there was blood, you would have thought of a different type of bank. You would have pushed the meaning of this word bank to the right space.",
  "transcript_length": 26028,
  "synthesis": {
    "raw_text": "**Core Thesis**\nThe attention mechanism in large language models is a crucial component that helps derive the meaning of words by adding context from other words in the same sentence, enabling the model to generate more accurate and relevant outputs.\n\n**Key Insights**\n\n* The attention mechanism uses a dot product or cosine distance to find the relevance of one word to another, allowing the model to \"pull\" the word towards its correct meaning based on the context.\n* Static embeddings (1D arrays) for words are not enough, as they don't capture the nuances of language and require additional context from other words in the sentence to disambiguate ambiguous words like \"bank\".\n* The attention mechanism can be thought of as a way to add weights to each word's vector based on its relevance to the surrounding words, effectively changing the meaning of the word based on the context.\n* This concept applies not only to text but also to images and video, where context can be derived from the visual elements in addition to the text.\n* The attention mechanism is a pre-processing step that is typically generated through training a neural network to cluster similar words together.\n\n**Actionable Takeaways**\n\n* To improve the accuracy of large language models, it's essential to incorporate the attention mechanism to add context and disambiguate ambiguous words.\n* When generating outputs for complex queries, consider using the attention mechanism to derive more accurate meanings from words with multiple possible interpretations.\n* In tokenization and vector representation, the attention mechanism plays a crucial role in adding weights to each word's vector based on its relevance to the surrounding words.\n* To improve the performance of large language models, it's essential to have enough training data and compute power to generate high-quality attention scores.\n* When working with languages like Hindi or Japanese, consider the nuances of these languages and how they impact the attention mechanism, as context can be derived from different sources (e.g., images or video in addition to text).",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 26028,
    "synthesis_length": 2107,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 1433.127182,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}