{
  "status": "success",
  "media_file": "https://youtu.be/_Y3BfN9v3sA?si=tdt7VwS4Ez7bk7Ul",
  "audio_file": "/var/folders/g1/nd4gy981295dn1ts1pjcb61c0000gn/T/media_knowledge_cr2yd5an/The_latest_LLM_research_shows_how_they_are_getting_youtube.wav",
  "transcript": "Hi everyone, in this video we'll see how large language models are being scaled up to become smarter and more efficient. So by the end of this video you'll know about the scaling laws for large language models and you'll also see what kind of advancements are being made in the current research to bring down the size of these models and also make them smarter at the same time. So stick around you'll learn a lot, let's start. There are two important scaling laws that we should keep in mind. The first one is that as these models get bigger they get smarter. So when I say bigger I mean the number of parameters that every large language model has. You have seen a neural network, the typical diagram of a bunch of neurons connected to each other. All of these connections are called edges and the weight of these edges is called a parameter. If you have more parameters, if you have more layers, if you have more connections, that's a more dense network, that's a bigger neural network. Is this smarter than something which is simpler with lesser weights? In general, yes. The Lama 405 billion parameter model will be much smarter than a Lama 3 billion parameter model. Basically all of the edges can map to a more dimensional space. If you have complex queries which are being asked to this model, it will be able to understand them through this complex n-dimensional space where it has many points, many parameters. This small puny model will not be able to map the input query rightly and it's going to lose compared to this model. So this suggests that we should use larger models possibly with trillions of parameters. The problem though is that when you have a very large model, training it is very expensive and slow because every input actually goes through this entire model with 405 billion parameters or trillions of parameters and after the output comes out you want to actually use it for back propagation. So again these weights are being continuously updated. So one idea is why don't we just simplify the weights of this model? Instead of using 32 bits or 64 bits to represent every weight like a floating point, we can instead just have one bit on every weight. So that's either 1 or 0. If you try this, it doesn't work out well, there's too much accuracy loss but if you change this slightly, if you make it 2 bits, so you have a signed 1 bit which is minus 1 or plus 1, suddenly this model becomes like really good. It's not as good as the 32 bit model but it's still retaining a lot of the intelligence with a 30x efficiency increase. And this result has been independently verified by both Microsoft and by Dan's. It's interesting to think of that you know you don't really need a lot of precision when you're looking at these weights, you just need an idea of whether I should keep the weight or I should ignore the weight or I should go against the weight. And the second thing you want to do with this model is you want to avoid fetching the weights, fetching the parameters of this model from disk from time to time. In fact because these models are being trained on a GPU, you don't even want the weights to be fetched from the CPU because that is like an IOCOL. What do you do? One thing you can do here is try to use the GPU cache efficiently and there's an idea around this which is basically flash attention. It utilizes the understanding of GPU hardware to make the cache more efficient, okay, the SRAM of the GPU more efficient. The model weights are stored there in a better way, the computations happen kind of in memory, so when you get an output it is much faster than it would if you didn't use this cache properly. So with these two ideas of reduce the size of the weights and try to store them in cache as much as you can, you're actually able to make the models much faster. And so you can invest this time saving into making the model better. How so? How do you make the model better? You saved some time but how does that help you make the model better? Well you can basically take samples or take the data points that you have and run them through the model and ask the model to generate multiple outputs and amongst the three choose the best one. Pick that and use that for reinforcement. And one remarkable example of this is from Shanghai University where researchers took a 1 billion parameter model which is a reasonably small model and actually beat a 405 billion parameter model. Okay I'm talking about Lama 1 billion parameters versus Lama 405 billion parameters, Lama 1 billion wins. How? It makes no sense. More parameters should make the model smarter. Well the amount of test time here was much larger. So every test, every data point which is input into this model during training generated multiple outputs for every data point, chose the best output, used that for reinforcement and this model is therefore much smarter although it's gone through the same number of data points as this one. The only difference is that this has been able to think more per data point than this one. And so this is our second scaling law. The more time you spend per data point during training the smarter your model gets. Okay our first scaling law was that the more parameters we have the smarter our model gets. Second one is the more time you spend per data point. Which one do you think is more important? Which one do you think brings more promise? This one right? The slope of scaling here is much larger. If I double the time per data point that I spend the intelligence is going to go a lot higher than if I double the number of parameters in a model. It makes more sense as an engineer to focus on this side than on this side. So for example, OpenAI says that for every data point that I have if I generate a really long response with long chain of thought then what I'm doing is I'm taking every data point and judiciously using it in my training. So during actual query time I'm going to come up with smarter more intelligent responses. Now this generating this long answer during training takes time. How do you get that time? By having savings in other places. Okay so this entire process of taking a data point, running it through the model and doing the back propagation can we make this faster? Yes. You don't have to update all the weights of the transformer. You just update the parts which are relevant to this output. So whenever you're sending in a data point you look at the parts which were enlightened or the parts which were affected. Only those parts are actually going to go through a retraining. That's an interesting thought process and it basically reduces the time that a model takes to go through different inputs. The second idea is from Google which says that it's not that every data point is important. Every data point is useful. Unless you have a lot of information in that data point you don't need to update the model. So think about a person saying the sun will rise from the east tomorrow. So what? I know the sun is going to rise from the east tomorrow. This is data. This is not information. This is useless. But if someone says the sun is going to rise from the west tomorrow, whoa okay I need to update my entire model because this piece of data has a lot of information, has high entropy, has surprise in it. So take those data points which have surprise or find out how much surprise every data point has. Based on that you have some sort of an investment in how much of the model do you want to change. How much of your worldview will change with the sun rising from the west tomorrow is very different from how much your worldview changes with the sun rising from the east tomorrow. Apart from this there's one other thing you can do to make the LLM go faster and that's basically look at the core of the LLM. Look at what it's made of. It's made of transformers and what's the thing about transformers? They need attention. Right? Attention is all you need. This is an order n square operation. Can't we make it faster? Very recently now MIT has released a paper called Lowell cats written by Simran and Rahul. They are back. The idea is very interesting. You're basically using these different operators to move away from attention which is very expensive to a new kind of order n operation which approximates that relation. So that's how large language models are being made smarter and smaller. In future we are likely to see many of these developments being used in large language models as a standard. Right? So if you're an AI engineer some of these things are useful to look into. I have linked all the research papers in the description. Thank you for watching this. If you have any doubts or suggestions you can let me know in the comments. I'll see you next time. Bye-bye. There's one thing I wanted to discuss and if you're looking for large language models at this point you can stop. You can go ahead and walk into the sunset. But if you're looking for something which is probably 10 years away or even more here's a topic. It's called neuromorphic computing. Right? We discussed that Google has come up with this paper which is entropy surprise. So if you see new information you are interested. If you see information which you don't care about it doesn't really matter. You don't update the model. We also talked about Sakana which says that if you see something then you should just update those parts of the model which are related to it. Guess what? The human brain already does this. The human brain looks at those parts which are being affected when someone raises their hand. It's not that my entire brain processes the entire image. It's a part of my brain processes a part of the image. So it's really energy efficient. Humans are incredible in that way. We have a few hundred calories and we are able to generate very good responses as compared to large language models. The reason for this is because like I said these two optimizations exist. The other thing is we compute things in memory. We don't have an ALU and a memory unit somewhere. So when we are taking in any kind of computation 2 plus 2 we don't send that 2 data to the memory. Bring it back to the ALU. Do the computation and send it back to memory. No. 2 plus 2 comes into our brain. It is processed in place and the response comes out in place. So that makes our brain again more efficient and finally our brain is incredible in the way that it doesn't work by binary. There's no zeros and ones. We have chemicals which are analog signals and the benefit of having chemicals in our brain or analog signals is they can be extremely precise. There's no zeros and ones. You don't need a very verbose language to say one word. Our brain can just represent that through a single state. Neuromorphic computing is basically taking these three ideas and trying to bring them into computers. So this is taking the hardware the GPUs that are currently being used in large language models and replacing them with brain inspired chips. It sounds very flashy but in reality it's not that crazy. Brain inspired could be anything. It could be things which are computing in memory. So that's one improvement. Non-binary chips. They're still digital but non-binary. Every point can have 16,000 states. Instead of just having two states of zero and one you now have a molecule where if you excite the oxygen then it's going to jump from one place to another. You note that down and you say oh this is a new state and then when it gets slightly less excited you say oh yeah there's another state here and then you might excite the particle in a different way. So you have 16,000 orientations and every orientation can be basically one data point instead of just zeros and ones which is voltage. So this is much more efficient again. And so if you're looking to learn more about this you can look at neuromorphic computing. I have the links in the description. I am particularly excited about it but I know that it's going to take a while to come up before we can replace GPUs with brain inspired chips. So until next time now see you. Bye.",
  "transcript_length": 12107,
  "synthesis": {
    "raw_text": "**Core Thesis**\n\nThe core thesis of this transcript is that large language models can be made smarter and smaller by applying two key scaling laws: increasing the number of parameters and spending more time per data point during training, while also optimizing computational efficiency through techniques such as reducing weight precision, utilizing GPU cache efficiently, and approximating attention mechanisms.\n\n**Key Insights**\n\n* Increasing the number of parameters in a large language model can lead to increased intelligence, but the law of diminishing returns applies, and further gains may be small.\n* Spending more time per data point during training is a more promising approach than increasing the number of parameters, as it allows the model to \"think\" more about each input.\n* Techniques such as reducing weight precision (e.g., from 32 bits to 2 bits) can significantly improve computational efficiency while retaining most of the model's intelligence.\n* Approximating attention mechanisms can also improve efficiency by reducing the computational complexity of attention operations.\n* Neuromorphic computing, which aims to replicate the efficiency and flexibility of biological brains in electronic hardware, holds promise for further improving large language models' performance.\n\n**Actionable Takeaways**\n\n* When training a large language model, focus on spending more time per data point rather than simply increasing the number of parameters.\n* Experiment with techniques like reduced weight precision (e.g., from 32 bits to 2 bits) to improve computational efficiency without sacrificing too much intelligence.\n* Consider using approximations of attention mechanisms to reduce computational complexity.\n* Invest in understanding and applying neuromorphic computing principles to develop more efficient and flexible hardware for large language models.\n* Evaluate the potential benefits of implementing GPU cache-efficient architectures, such as flash attention, to optimize model performance.",
    "model_used": "llama3.1:8b",
    "template_used": "basic_summary",
    "transcript_length": 12107,
    "synthesis_length": 2011,
    "use_cloud": false
  },
  "model_used": "llama3.1:8b",
  "template_used": "basic_summary",
  "processing_time": 150.962693,
  "error": null,
  "is_playlist": false,
  "playlist_results": null
}