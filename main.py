#!/usr/bin/env python3
"""
Media-to-Knowledge Pipeline - Main Orchestrator

This script processes video or audio files through a pipeline:
1. Extract/prepare audio from media file
2. Transcribe audio to text using Whisper
3. Synthesize knowledge from transcript using Ollama

Usage:
    python main.py --input /path/to/media.mp4
    python main.py --input /path/to/audio.mp3 --cloud
    python main.py --input /path/to/video.mov --prompt meeting_minutes
"""

import argparse
import json
import sys
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

from core.media_preprocessor import prepare_audio, MediaPreprocessorError
from core.transcriber import transcribe_audio, TranscriberError
from core.synthesizer import KnowledgeSynthesizer, SynthesizerError
from config import get_config


def print_separator(char: str = "=", length: int = 80) -> None:
    """Print a separator line."""
    print(char * length)


def print_section(title: str) -> None:
    """Print a section header."""
    print_separator()
    print(f"\n{title}\n")
    print_separator()


def save_results_to_file(results: Dict[str, Any], output_path: str) -> None:
    """
    Save results to a JSON file.
    
    Args:
        results: Dictionary containing pipeline results.
        output_path: Path to the output file.
    """
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n✓ Results saved to: {output_path}")


def save_synthesis_to_markdown(results: Dict[str, Any], output_dir: str = "outputs/markdown") -> None:
    """
    Save synthesized text to a markdown file with subject as filename.
    
    Args:
        results: Dictionary containing pipeline results.
        output_dir: Directory to save markdown files (default: "outputs/markdown").
    """
    if results["status"] != "success" or not results["synthesis"]:
        return
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Extract synthesis text
    synthesis_text = results["synthesis"]["raw_text"]
    
    # Generate filename from subject (first few words of synthesis)
    # Remove problematic characters and limit length
    first_line = synthesis_text.split('\n')[0].strip('#* ')
    subject = first_line[:50].strip() if first_line else "knowledge_synthesis"
    # Clean filename
    filename = "".join(c for c in subject if c.isalnum() or c in (' ', '-', '_')).rstrip()
    filename = filename.replace(' ', '_').lower()
    # Ensure we have a valid filename
    if not filename or filename.strip() == "":
        filename = "knowledge_synthesis"
    
    # Create markdown content
    md_content = f"""# {subject}

## Source
- File: {results.get('media_file', 'Unknown')}
- Processing Time: {results.get('processing_time', 0):.2f} seconds
- Transcript Length: {results.get('transcript_length', 0):,} characters
- Model Used: {results.get('model_used', 'Unknown')}

## Synthesized Knowledge

{synthesis_text}

---
*Generated by Media-to-Knowledge Pipeline*
"""
    
    # Handle duplicate filenames by adding timestamp
    md_file_path = output_path / f"{filename}.md"
    if md_file_path.exists():
        # Add timestamp for uniqueness
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        md_file_path = output_path / f"{filename}_{timestamp}.md"
    
    with open(md_file_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"✓ Synthesis saved to markdown: {md_file_path}")




def process_media(
    media_path: str,
    use_cloud_synth: bool = False,
    prompt_template: Optional[str] = None,
    custom_prompt: Optional[str] = None
) -> Dict[str, Any]:
    """
    Process a media file through the complete pipeline.
    
    This function orchestrates the entire media-to-knowledge pipeline:
    1. Prepare audio from media file (extract or convert)
    2. Transcribe audio to text using Whisper
    3. Synthesize knowledge from transcript using Ollama
    
    Args:
        media_path: Path to video or audio file.
        use_cloud_synth: Whether to use Ollama Cloud for synthesis.
        prompt_template: Optional prompt template key (e.g., 'meeting_minutes').
        custom_prompt: Optional custom prompt text (overrides prompt_template).
    
    Returns:
        Dictionary containing:
            - status: 'success' or 'error'
            - media_file: Path to input media file
            - audio_file: Path to prepared audio file
            - transcript: Transcribed text
            - transcript_length: Length of transcript
            - synthesis: Synthesized knowledge
            - model_used: Ollama model used
            - template_used: Prompt template used
            - processing_time: Total processing time in seconds
            - error: Error message if status is 'error'
    
    Raises:
        MediaPreprocessorError: If media preprocessing fails.
        TranscriberError: If transcription fails.
        SynthesizerError: If synthesis fails.
    
    Example:
        >>> results = process_media("video.mp4", use_cloud_synth=False)
        >>> print(results["synthesis"]["raw_text"])
    """
    start_time = datetime.now()
    results = {
        "status": "success",
        "media_file": media_path,
        "audio_file": None,
        "transcript": None,
        "transcript_length": 0,
        "synthesis": None,
        "model_used": None,
        "template_used": None,
        "processing_time": 0,
        "error": None
    }
    
    # Create temporary directory for intermediate files
    with tempfile.TemporaryDirectory(prefix="media_knowledge_") as temp_dir:
        try:
            # Step 1: Prepare audio from media file
            print_section("STEP 1: Media Preprocessing")
            print(f"Input file: {media_path}")
            
            audio_path = prepare_audio(media_path, temp_dir)
            results["audio_file"] = audio_path
            print(f"✓ Audio prepared: {audio_path}")
            
            # Step 2: Transcribe audio to text
            print_section("STEP 2: Speech-to-Text Transcription")
            
            config = get_config(use_cloud=False)
            transcript = transcribe_audio(
                audio_path,
                model_size=config.whisper_model_size
            )
            results["transcript"] = transcript
            results["transcript_length"] = len(transcript)
            print(f"✓ Transcription complete: {len(transcript)} characters")
            
            # Step 3: Synthesize knowledge from transcript
            print_section("STEP 3: Knowledge Synthesis")
            
            synthesizer = KnowledgeSynthesizer(use_cloud=use_cloud_synth)
            
            # Test connection before synthesis
            if not synthesizer.test_connection():
                raise SynthesizerError(
                    f"Cannot connect to Ollama. "
                    f"Ensure Ollama is running: 'ollama serve'"
                )
            
            synthesis_result = synthesizer.synthesize(
                transcript=transcript,
                prompt_template=prompt_template,
                custom_prompt=custom_prompt
            )
            
            results["synthesis"] = synthesis_result
            results["model_used"] = synthesis_result["model_used"]
            results["template_used"] = synthesis_result["template_used"]
            
            print(f"✓ Synthesis complete: {synthesis_result['synthesis_length']} characters")
            
        except (MediaPreprocessorError, TranscriberError, SynthesizerError) as e:
            results["status"] = "error"
            results["error"] = str(e)
            print(f"\n✗ Error: {e}")
            return results
    
    # Calculate total processing time
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    results["processing_time"] = processing_time
    
    return results


def display_results(results: Dict[str, Any]) -> None:
    """
    Display pipeline results in a readable format.
    
    Args:
        results: Dictionary containing pipeline results.
    """
    print_section("PIPELINE RESULTS")
    
    if results["status"] == "error":
        print(f"Status: ✗ Failed")
        print(f"Error: {results['error']}")
        return
    
    print(f"Status: ✓ Success")
    print(f"Processing Time: {results['processing_time']:.2f} seconds")
    print(f"\nInput File: {results['media_file']}")
    print(f"Audio File: {results['audio_file']}")
    print(f"Transcript Length: {results['transcript_length']:,} characters")
    print(f"Model Used: {results['model_used']}")
    print(f"Template Used: {results['template_used']}")
    
    if results["synthesis"]:
        print_section("SYNTHESIZED KNOWLEDGE")
        print(results["synthesis"]["raw_text"])
    
    print_separator()


def main():
    """Main entry point with CLI interface."""
    parser = argparse.ArgumentParser(
        description="Media-to-Knowledge Pipeline: Extract and synthesize knowledge from video/audio files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process a video file with local Ollama
  python main.py --input video.mp4
  
  # Process an audio file with cloud Ollama
  python main.py --input audio.mp3 --cloud
  
  # Use a specific prompt template
  python main.py --input meeting.mp4 --prompt meeting_minutes
  
  # Save results to a JSON file
  python main.py --input lecture.mp4 --output results.json
  
  # Save synthesis to markdown file
  python main.py --input lecture.mp4 --markdown outputs/markdown
  
  # Save both JSON and markdown
  python main.py --input lecture.mp4 --output results.json --markdown outputs/markdown
  
  # Use a custom prompt
  python main.py --input interview.mp4 --prompt "Summarize the key points: {transcript}"

Available prompt templates:
  basic_summary, meeting_minutes, lecture_summary, tutorial_guide,
  project_update, customer_feedback, research_summary, interview_summary,
  blog_post_outline, social_media_content, technical_documentation, bug_report_summary
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        required=True,
        help="Path to video or audio file to process"
    )
    
    parser.add_argument(
        "--cloud",
        action="store_true",
        help="Use Ollama Cloud for knowledge synthesis (default: local)"
    )
    
    parser.add_argument(
        "--prompt", "-p",
        help="Prompt template key (e.g., 'meeting_minutes') or custom prompt text"
    )
    
    parser.add_argument(
        "--output", "-o",
        help="Optional output file path for results (JSON format)"
    )
    
    parser.add_argument(
        "--markdown", "-m",
        help="Optional directory path for markdown output (default: outputs/markdown)"
    )
    
    parser.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress detailed output, only show final results"
    )
    
    args = parser.parse_args()
    
    # Validate input file exists
    if not Path(args.input).exists():
        print(f"Error: Input file not found: {args.input}", file=sys.stderr)
        sys.exit(1)
    
    # Print header
    if not args.quiet:
        print_separator()
        print("Media-to-Knowledge Pipeline")
        print_separator()
        print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print_separator()
    
    # Process the media file
    try:
        results = process_media(
            media_path=args.input,
            use_cloud_synth=args.cloud,
            prompt_template=args.prompt if args.prompt and not args.prompt.startswith("Summarize") else None,
            custom_prompt=args.prompt if args.prompt and args.prompt.startswith("Summarize") else None
        )
        
        # Display or save results
        if args.output:
            save_results_to_file(results, args.output)
            if not args.quiet:
                display_results(results)
        elif args.markdown:
            # Save to markdown only
            save_synthesis_to_markdown(results, args.markdown)
            if not args.quiet:
                display_results(results)
        else:
            display_results(results)
        
        # Save to markdown if requested (in addition to JSON)
        if args.markdown and args.output:
            save_synthesis_to_markdown(results, args.markdown)
        
        # Exit with appropriate code
        if results["status"] == "error":
            sys.exit(1)
        else:
            if not args.quiet:
                print(f"\n✓ Pipeline completed successfully!")
                print(f"Total processing time: {results['processing_time']:.2f} seconds")
            sys.exit(0)
            
    except KeyboardInterrupt:
        print("\n\n✗ Pipeline interrupted by user", file=sys.stderr)
        sys.exit(130)
    except Exception as e:
        print(f"\n✗ Unexpected error: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()